import torch
import torch.nn as nn
import torch.optim as optim
import random
import matplotlib.pyplot as plt
import sys
sys.path.append('..')
from models.model_parts import CausalSelfAttention

# Device Config
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running on: {device}")

# ==========================================
# 1. DECIMAL DATA GENERATOR
# ==========================================
def generate_decimal_batch(batch_size, length):
    X, Y = [], []
    
    # We always define the sequence length as N + 1
    # This guarantees space for the final carry.
    seq_len = length + 1
    
    for _ in range(batch_size):
        # 1. Generate random numbers of 'length' digits
        # e.g., if length=2, max is 99.
        max_val = (10 ** length) - 1
        a = random.randint(0, max_val)
        b = random.randint(0, max_val)
        c = a + b # Real sum (e.g., 99+99=198)
        
        # 2. Extract digits for the FULL sequence (length + 1)
        # We grab digits 0 to 'length'.
        # If the number is small (e.g. 50+20=70), the last digit will naturally be 0.
        a_seq = [(a // (10**i)) % 10 for i in range(seq_len)]
        b_seq = [(b // (10**i)) % 10 for i in range(seq_len)]
        c_seq = [(c // (10**i)) % 10 for i in range(seq_len)]
        
        # 3. Stack inputs
        # The last position of input will always be [0, 0]
        # The AI sees this and thinks "Ah, a blank space for me to write the carry."
        x_seq = [[d_a, d_b] for d_a, d_b in zip(a_seq, b_seq)]
        
        X.append(x_seq)
        Y.append(c_seq)
        
    return torch.tensor(X).long().to(device), torch.tensor(Y).long().to(device)


# Vocabulary mapping, 0 and 1 are binary digits.
TOK = {
    "0": 0, "1": 1,
    "+": 2,
    ">": 3,
    "#": 4,
    "*": 5,   # only used in y to mark "ignored" positions conceptually
}
# TODO: Implement the binary addition batch generator based on paper to see if paper tricks help other transformers generalize and if the reason they generalize is due to their syntax trick in input!!!

def generate_binary_addition_batch(batch_size, length, max_seq_length=25, device='cpu'):
    """
    Generates a batch of binary addition tasks as described in Section 6.1.1 
    of the Looped Transformers paper for Length Generalization.
    
    Notes on the format:
    - Order: Standard (MSD first), explicitly NOT reversed 
    - Format: "summand1 + summand2 > answer" (Implicit FOP structure) 
    - Input: looks like "x1...operator...xn>#...#" or "10+11>###".
    - Output: looks like "*...*x1...xn#" or "*****101##".
    """
    
    
    X_batch = []
    Y_batch = []

    for _ in range(batch_size):
        # 1. Generate numbers
        # The paper defines length as the number of digits in the summands
        # Length here is max int represented in binary (e.g. 2 bits -> 3, 3 bits -> 7, 8 bits -> 255)
        max_val = (2 ** length) - 1
        a_int = random.randint(0, max_val)
        b_int = random.randint(0, max_val)
        c_int = a_int + b_int

        # 2. Convert to Binary Lists (Standard Order / Big-Endian)
        # The paper explicitly states: "we do not reverse the output".
        # We enforce fixed length padding for inputs to match the paper's "n digits
        def to_bin(val, width):
            bin_str = f"{val:0{width}b}"
            return [int(d) for d in bin_str]

        a_seq = to_bin(a_int, length)
        b_seq = to_bin(b_int, length)
        # Output has one more digit than inputs.
        c_seq = to_bin(c_int, length) # in paper it is length + 1, but that felt like cheating

        # 3. Construct Sequences
        # Format: Input 1 + Input 2 > Answer # 
        
        # The Query (Input) part: "011+010>"
        query = a_seq + [TOK["+"]] + b_seq
        if len(query) >= max_seq_length:
            raise ValueError(f"Generated query length {len(query)} should be less than max_seq_length {max_seq_length}")
        query = query + ([TOK["#"]] * (max_seq_length - len(query)))
        
        # The Answer part: "101#"
        answer = c_seq 
        if len(answer) >= max_seq_length:
            raise ValueError(f"Generated answer length {len(answer)} should be less than max_seq_length {max_seq_length}")
        answer = answer + ([TOK["#"]] * (max_seq_length - len(answer)))
        
        # # 4. Construct Full Tensors for FOP (Full Output Prediction)
        # # In FOP, the input contains the query + placeholders for the answer[cite: 103].
        # # The target contains the answer at the correct positions.
        
        # # Input: [Query] [EOS placeholders]
        # # Paper says: "rest of the locations are filled with multiple EOS tokens"[cite: 103].
        # input_seq = query + [TOK[">"]] + ([TOK["#"]] * len(answer))
        
        # # Target: [Ignored Query] [Answer]
        # # In standard PyTorch CrossEntropy, we use -100 to ignore targets.
        # # The paper says output locations before EOQ are ignored[cite: 104].
        # target_seq = [-100] * (len(query)) + answer + ([TOK["#"]] * 2)

        X_batch.append(query)
        Y_batch.append(answer)

    return torch.tensor(X_batch).long().to(device), torch.tensor(Y_batch).long().to(device)

# ==========================================
# 2. STANDARD DECIMAL TRANSFORMER
# ==========================================
class BaseTransformer(nn.Module):
    def __init__(self, d_model, nhead, num_layers=4, max_len=100):
        super().__init__()
        # Embedding for digits 0-9
        # self.digit_embedding = nn.Embedding(10, d_model)
        self.digit_embedding_a = nn.Embedding(10, d_model // 2)
        self.digit_embedding_b = nn.Embedding(10, d_model // 2)
        
        # Absolute Positional Encoding
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.1)
        
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True, norm_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Output 10 classes (digits 0-9)
        self.output_head = nn.Linear(d_model, 10)

    def forward(self, x, num_loops=None):
        seq_len = x.shape[1]
        
        # Embed the two input digits separately and sum them
        # x[:,:,0] is digit A, x[:,:,1] is digit B
        # emb_a = self.digit_embedding(x[:,:,0])
        # emb_b = self.digit_embedding(x[:,:,1])
        # h = emb_a + emb_b # Combine
        emb_a = self.digit_embedding_a(x[:,:,0])
        emb_b = self.digit_embedding_b(x[:,:,1])
        h = torch.cat([emb_a, emb_b], dim=-1)
        
        # Add position
        h = h + self.pos_embedding[:, :seq_len, :]
        
        h = self.transformer(h)
        return self.output_head(h)
    
class BaseTransformer2(nn.Module):
    def __init__(self, d_model, nhead, num_layers=40, max_len=100, use_rope=True, causal=True, rope_type='2d'):
        super().__init__()
        self.digit_embedding = nn.Embedding(10, d_model)
        self.digit_embedding_a = nn.Embedding(10, d_model // 2)
        self.digit_embedding_b = nn.Embedding(10, d_model // 2)
        self.use_rope = use_rope

        if not use_rope:
            self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.1)

        self.layers = nn.ModuleList([
            nn.ModuleDict({
                'attn': CausalSelfAttention(d_model=d_model, n_head=nhead, max_len=max_len, use_rope=use_rope, causal=causal, rope_type=rope_type),
                'ffn': nn.Sequential(
                    nn.Linear(d_model, 2048),
                    nn.ReLU(),
                    nn.Linear(2048, d_model)
                ),
                'ln1': nn.LayerNorm(d_model),
                'ln2': nn.LayerNorm(d_model)
            })
            for _ in range(num_layers)
        ])
        self.output_head = nn.Linear(d_model, 10)

        # Should we try implementing relative bias??
        
    def get_relative_bias(self, seq_len, device):
        range_vec = torch.arange(seq_len, device=device)
        distance_mat = range_vec[None, :] - range_vec[:, None]
        distance_mat_clamped = torch.clamp(distance_mat, -self.max_relative_dist, self.max_relative_dist)
        final_indices = distance_mat_clamped + self.max_relative_dist
        bias = self.relative_bias_table[0, :, final_indices.long()]
        causal_mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)
        return bias + causal_mask.unsqueeze(0)

    def forward(self, x, num_loops=1):
        # emb_a = self.digit_embedding(x[:,:,0])
        # emb_b = self.digit_embedding(x[:,:,1])
        # h = emb_a + emb_b
        
        emb_a = self.digit_embedding_a(x[:,:,0])
        emb_b = self.digit_embedding_b(x[:,:,1])
        input_emb = torch.cat([emb_a, emb_b], dim=-1)
        h = input_emb.clone()

        if not self.use_rope:
            h = h + self.pos_embedding[:, :x.shape[1], :]

        for i in range(num_loops):
            for layer in self.layers:
                h = h + layer['attn'](layer['ln1'](h))
                h = h + layer['ffn'](layer['ln2'](h))
                h = h + input_emb

        return self.output_head(h)

# ==========================================
# 3. LOOPED DECIMAL TRANSFORMER
# ==========================================
class LoopedTransformer(nn.Module):
    def __init__(self, d_model, nhead, max_relative_dist=4):
        super().__init__()
        self.digit_embedding_a = nn.Embedding(10, d_model // 2)
        self.digit_embedding_b = nn.Embedding(10, d_model // 2)
        
        # Relative Bias
        self.max_relative_dist = max_relative_dist
        self.relative_bias_table = nn.Parameter(torch.Tensor(1, nhead, 2 * max_relative_dist + 1))
        nn.init.xavier_uniform_(self.relative_bias_table)
        
        self.shared_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, batch_first=True, norm_first=True
        )
        self.output_head = nn.Linear(d_model, 10)
        self.input_gate = nn.Parameter(torch.tensor(0.1))

    def get_relative_bias(self, seq_len, device):
        range_vec = torch.arange(seq_len, device=device)
        distance_mat = range_vec[None, :] - range_vec[:, None]
        distance_mat_clamped = torch.clamp(distance_mat, -self.max_relative_dist, self.max_relative_dist)
        final_indices = distance_mat_clamped + self.max_relative_dist
        bias = self.relative_bias_table[0, :, final_indices.long()]
        causal_mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)
        return bias + causal_mask.unsqueeze(0)

    def forward(self, x, num_loops=None):
        B, seq_len, _ = x.shape
        if num_loops is None:
            num_loops = seq_len + 5

        # Embed Inputs
        emb_a = self.digit_embedding_a(x[:,:,0])
        emb_b = self.digit_embedding_b(x[:,:,1])
        input_emb = torch.cat([emb_a, emb_b], dim=-1)
        h = input_emb.clone()
        
        relative_bias = self.get_relative_bias(seq_len, x.device)
        attn_mask = relative_bias.repeat(B, 1, 1)

        for _ in range(num_loops):
            h_new = self.shared_layer(h, src_mask=attn_mask)
            h = h_new + (self.input_gate * input_emb)
            
        return self.output_head(h)

# ==========================================
# 4. TRAINING & PLOTTING
# ==========================================
def train_and_compare():
    D_MODEL = 256 # Slightly larger for decimal complexity
    NHEAD = 4
    
    # 1. Setup Models
    std_model = BaseTransformer(D_MODEL, NHEAD, num_layers=4).to(device)
    loop_model = BaseTransformer2(D_MODEL, NHEAD, num_layers=4, use_rope=True, causal=True, rope_type='2d').to(device) # Window=4

    optimizers = {
        "Standard": optim.Adam(std_model.parameters(), lr=0.001),
        "Looped": optim.Adam(loop_model.parameters(), lr=0.001)
    }
    
    criterion = nn.CrossEntropyLoss()
    
    print("--- Training Phase (8-Digit Numbers) ---")
    # Train both for 2000 steps
    for step in range(2001):
        X, Y = generate_decimal_batch(64, length=8)
        
        # Train Standard
        opt = optimizers["Standard"]
        opt.zero_grad()
        out = std_model(X)
        loss_std = criterion(out.reshape(-1, 10), Y.reshape(-1))
        loss_std.backward()
        opt.step()
        
        # Train Looped
        opt = optimizers["Looped"]
        opt.zero_grad()
        out = loop_model(X, num_loops=4) # 8 digits + buffer
        loss_loop = criterion(out.reshape(-1, 10), Y.reshape(-1))
        loss_loop.backward()
        opt.step()
        
        if step % 500 == 0:
            print(f"Step {step}: Std Loss {loss_std.item():.3f} | Loop Loss {loss_loop.item():.3f}")

    print("\n--- Generalization Test (Length 8 to 24) ---")
    lengths = [8, 12, 16, 20, 24, 30, 32,35, 40]
    std_accs = []
    loop_accs = []
    
    for L in lengths:
        with torch.no_grad():
            X_test, Y_test = generate_decimal_batch(100, length=L)
            
            # Test Standard
            out_std = std_model(X_test)
            acc_s = (out_std.argmax(-1) == Y_test).float().mean().item()
            std_accs.append(acc_s)
            
            # Test Looped (Scale loops with length)
            out_loop = loop_model(X_test, num_loops=L+4)
            acc_l = (out_loop.argmax(-1) == Y_test).float().mean().item()
            loop_accs.append(acc_l)
            
    # Print Table
    print(f"{'Length':<10} | {'Standard Acc':<15} | {'Looped Acc':<15}")
    print("-" * 45)
    for i, L in enumerate(lengths):
        print(f"{L:<10} | {std_accs[i]:.2f}{'':<11} | {loop_accs[i]:.2f}")

    # Plot
    plt.figure(figsize=(8, 5))
    plt.plot(lengths, std_accs, 'r-o', label='Base Transformer')
    plt.plot(lengths, loop_accs, 'g-o', label='Base Transformer (causal)')
    plt.axvline(x=8, color='gray', linestyle='--', label='Training Cutoff')
    plt.title("Decimal Addition: Generalization to Long Numbers")
    plt.xlabel("Number of Digits")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    return std_model, loop_model

std_model, loop_model = train_and_compare()