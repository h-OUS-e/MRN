'''
Attempting to test this paper and replicate results to understand
how Looped Transformers work and why they generalize better.

Paper: https://arxiv.org/pdf/2409.15647
Github: https://github.com/UW-Madison-Lee-Lab/looped-tf
'''

import torch
import torch.nn as nn
import torch.optim as optim
import random
import matplotlib.pyplot as plt
import sys
sys.path.append('..')
from models.model_parts import CausalSelfAttention

# Device Config
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running on: {device}")

# ==========================================
# 1. DECIMAL DATA GENERATOR
# ==========================================
def generate_decimal_batch(batch_size, length):
    """
    Generates a batch of decimal addition/subtraction problems.
    
    Args:
        batch_size: Number of examples to generate
        length: Number of digits in each operand (e.g., length=3 means 0-999)
    
    Returns:
        X: [batch_size, seq_len, 2] - Input pairs of digits (digit_a, digit_b) at each position
        Y: [batch_size, seq_len] - Target output digits (result of operation)
    
    Format:
        - Digits stored in little-endian (least significant digit first)
        - seq_len = length + 1 to accommodate carry/borrow
        - X[i,j] = [a's j-th digit, b's j-th digit]
        - Y[i,j] = result's j-th digit
    """
    X, Y = [], []
    
    # We always define the sequence length as N + 1
    # This guarantees space for the final carry.
    seq_len = length + 1
    
    for _ in range(batch_size):
        # 1. Generate random numbers of 'length' digits
        # e.g., if length=2, max is 99.
        max_val = (10 ** length) - 1
        a = random.randint(0, max_val)
        b = random.randint(0, max_val)
        c = a + b # Real sum (e.g., 99+99=198)
        
        # 2. Extract digits for the FULL sequence (length + 1)
        # We grab digits 0 to 'length'.
        # If the number is small (e.g. 50+20=70), the last digit will naturally be 0.
        a_seq = [(a // (10**i)) % 10 for i in range(seq_len)]
        b_seq = [(b // (10**i)) % 10 for i in range(seq_len)]
        c_seq = [(c // (10**i)) % 10 for i in range(seq_len)]
        
        # 3. Stack inputs
        # The last position of input will always be [0, 0]
        # The AI sees this and thinks "Ah, a blank space for me to write the carry."
        x_seq = [[d_a, d_b] for d_a, d_b in zip(a_seq, b_seq)]
        
        X.append(x_seq)
        Y.append(c_seq)
        
    return torch.tensor(X).long().to(device), torch.tensor(Y).long().to(device)


# Vocabulary mapping, 0 and 1 are binary digits.
TOK = {
    "0": 0, "1": 1,
    "+": 2,
    ">": 3,
    "#": 4,
    "*": 5,   # only used in y to mark "ignored" positions conceptually
}
# TODO: Implement the binary addition batch generator based on paper to see if paper tricks help other transformers generalize and if the reason they generalize is due to their syntax trick in input!!!
def generate_binary_addition_batch(batch_size, length, max_seq_length=25, device='cpu'):
    """
    Generates a batch of binary addition tasks as described in Section 6.1.1 
    of the Looped Transformers paper for Length Generalization.
    
    Notes on the format:
    - Order: Standard (MSD first), explicitly NOT reversed 
    - Format: "summand1 + summand2 > answer" (Implicit FOP structure) 
    - Input: looks like "x1...operator...xn>#...#" or "10+11>###".
    - Output: looks like "*...*x1...xn#" or "*****101##".
    """    
    
    X_batch = []
    Y_batch = []

    for _ in range(batch_size):
        # 1. Generate numbers
        # The paper defines length as the number of digits in the summands
        # Length here is max int represented in binary (e.g. 2 bits -> 3, 3 bits -> 7, 8 bits -> 255)
        max_val = (2 ** length) - 1
        a_int = random.randint(0, max_val)
        b_int = random.randint(0, max_val)
        c_int = a_int + b_int

        # 2. Convert to Binary Lists (Standard Order / Big-Endian)
        # The paper explicitly states: "we do not reverse the output".
        # We enforce fixed length padding for inputs to match the paper's "n digits
        def to_bin(val, width):
            bin_str = f"{val:0{width}b}"
            return [int(d) for d in bin_str]

        a_seq = to_bin(a_int, length)
        b_seq = to_bin(b_int, length)
        # Output has one more digit than inputs.
        c_seq = to_bin(c_int, length) # in paper it is length + 1, but that felt like cheating

        # 3. Construct Sequences
        # Format: Input 1 + Input 2 > Answer # 
        
        # The Query (Input) part: "011+010>"
        query = a_seq + [TOK["+"]] + b_seq
        if len(query) >= max_seq_length:
            raise ValueError(f"Generated query length {len(query)} should be less than max_seq_length {max_seq_length}")
        query = query + ([TOK["#"]] * (max_seq_length - len(query)))
        
        # The Answer part: "101#"
        answer = c_seq 
        if len(answer) >= max_seq_length:
            raise ValueError(f"Generated answer length {len(answer)} should be less than max_seq_length {max_seq_length}")
        answer = answer + ([TOK["#"]] * (max_seq_length - len(answer)))
        
        # # 4. Construct Full Tensors for FOP (Full Output Prediction)
        # # In FOP, the input contains the query + placeholders for the answer[cite: 103].
        # # The target contains the answer at the correct positions.
        
        # # Input: [Query] [EOS placeholders]
        # # Paper says: "rest of the locations are filled with multiple EOS tokens"[cite: 103].
        # input_seq = query + [TOK[">"]] + ([TOK["#"]] * len(answer))
        
        # # Target: [Ignored Query] [Answer]
        # # In standard PyTorch CrossEntropy, we use -100 to ignore targets.
        # # The paper says output locations before EOQ are ignored[cite: 104].
        # target_seq = [-100] * (len(query)) + answer + ([TOK["#"]] * 2)

        X_batch.append(query)
        Y_batch.append(answer)

    return torch.tensor(X_batch).long().to(device), torch.tensor(Y_batch).long().to(device)

# ==========================================
# 2. STANDARD DECIMAL TRANSFORMER
# ==========================================
class BaseTransformer(nn.Module):
    def __init__(self, d_model, nhead, num_layers=4, max_len=100):
        super().__init__()
        # Embedding for digits 0-9
        self.digit_embedding = nn.Embedding(10, d_model)
        # self.digit_embedding_a = nn.Embedding(10, d_model // 2)
        # self.digit_embedding_b = nn.Embedding(10, d_model // 2)
        
        # Absolute Positional Encoding
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.1)
        
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True, norm_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Output 10 classes (digits 0-9)
        self.output_head = nn.Linear(d_model, 10)

    def forward(self, x, num_loops=None):
        seq_len = x.shape[1]
        
        # Embed the two input digits separately and sum them
        # x[:,:,0] is digit A, x[:,:,1] is digit B
        # emb_a = self.digit_embedding(x[:,:,0])
        # emb_b = self.digit_embedding(x[:,:,1])
        # h = emb_a + emb_b # Combine
        # emb_a = self.digit_embedding_a(x[:,:,0])
        # emb_b = self.digit_embedding_b(x[:,:,1])
        # h = torch.cat([emb_a, emb_b], dim=-1)
        h=self.digit_embedding(x)
        
        # Add position
        h = h + self.pos_embedding[:, :seq_len, :]
        
        h = self.transformer(h)
        return self.output_head(h)
    
class BaseTransformer2(nn.Module):
    def __init__(self, d_model, nhead, num_layers=4, max_len=100, use_rope=True, causal=True, rope_type='2d', dim_feedforward=None, dropout=0.0, activation='gelu', use_relative_bias=False, max_relative_dist=4):
        super().__init__()
        self.digit_embedding = nn.Embedding(10, d_model)
        # self.digit_embedding_a = nn.Embedding(10, d_model // 2)
        # self.digit_embedding_b = nn.Embedding(10, d_model // 2)
        self.use_rope = use_rope

        # Match PyTorch default: 4 * d_model
        if dim_feedforward is None:
            dim_feedforward = 4 * d_model

        if not use_rope:
            self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.1)

        # Choose activation function (PyTorch default is ReLU for TransformerEncoderLayer)
        act_fn = nn.GELU() if activation == 'gelu' else nn.ReLU()

        self.layers = nn.ModuleList([
            nn.ModuleDict({
                'attn': CausalSelfAttention(
                    d_model=d_model, 
                    n_head=nhead, 
                    max_len=max_len, 
                    use_rope=use_rope, 
                    causal=causal, 
                    rope_type=rope_type, 
                    dropout=dropout, 
                    use_relative_bias=use_relative_bias, 
                    max_relative_dist=max_relative_dist
                ),
                'ffn': nn.Sequential(
                    nn.Linear(d_model, dim_feedforward),
                    act_fn,
                    nn.Dropout(dropout),
                    nn.Linear(dim_feedforward, d_model),
                    nn.Dropout(dropout)
                ),
                'ln1': nn.LayerNorm(d_model),
                'ln2': nn.LayerNorm(d_model)
            })
            for _ in range(num_layers)
        ])
        self.output_head = nn.Linear(d_model, 10)

        # Should we try implementing relative bias??
        
    def get_relative_bias(self, seq_len, device):
        range_vec = torch.arange(seq_len, device=device)
        distance_mat = range_vec[None, :] - range_vec[:, None]
        distance_mat_clamped = torch.clamp(distance_mat, -self.max_relative_dist, self.max_relative_dist)
        final_indices = distance_mat_clamped + self.max_relative_dist
        bias = self.relative_bias_table[0, :, final_indices.long()]
        causal_mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)
        return bias + causal_mask.unsqueeze(0)

    def forward(self, x, num_loops=1):
        # emb_a = self.digit_embedding(x[:,:,0])
        # emb_b = self.digit_embedding(x[:,:,1])
        # h = emb_a + emb_b
        
        # emb_a = self.digit_embedding_a(x[:,:,0])
        # emb_b = self.digit_embedding_b(x[:,:,1])
        # input_emb = torch.cat([emb_a, emb_b], dim=-1)
        input_emb = self.digit_embedding(x)
        h = input_emb.clone()

        if not self.use_rope:
            h = h + self.pos_embedding[:, :x.shape[1], :]

        if num_loops is None:
            for layer in self.layers:
                h = h + layer['attn'](layer['ln1'](h))
                h = h + layer['ffn'](layer['ln2'](h))
            
        else:
            for i in range(num_loops):
                for layer in self.layers:
                    h = h + layer['attn'](layer['ln1'](h))
                    h = h + layer['ffn'](layer['ln2'](h))
                    h = h + input_emb

        return self.output_head(h)

# ==========================================
# 3. LOOPED DECIMAL TRANSFORMER
# ==========================================
class LoopedTransformer(nn.Module):
    def __init__(self, d_model, nhead, max_relative_dist=4):
        super().__init__()
        self.digit_embedding_a = nn.Embedding(10, d_model // 2)
        self.digit_embedding_b = nn.Embedding(10, d_model // 2)
        
        # Relative Bias
        self.max_relative_dist = max_relative_dist
        self.relative_bias_table = nn.Parameter(torch.Tensor(1, nhead, 2 * max_relative_dist + 1))
        nn.init.xavier_uniform_(self.relative_bias_table)
        
        self.shared_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, batch_first=True, norm_first=True
        )
        self.output_head = nn.Linear(d_model, 10)
        self.input_gate = nn.Parameter(torch.tensor(0.1))

    def get_relative_bias(self, seq_len, device):
        range_vec = torch.arange(seq_len, device=device)
        distance_mat = range_vec[None, :] - range_vec[:, None]
        distance_mat_clamped = torch.clamp(distance_mat, -self.max_relative_dist, self.max_relative_dist)
        final_indices = distance_mat_clamped + self.max_relative_dist
        bias = self.relative_bias_table[0, :, final_indices.long()]
        causal_mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)
        return bias + causal_mask.unsqueeze(0)

    def forward(self, x, num_loops=None):
        B, seq_len, _ = x.shape
        if num_loops is None:
            num_loops = seq_len + 5

        # Embed Inputs
        emb_a = self.digit_embedding_a(x[:,:,0])
        emb_b = self.digit_embedding_b(x[:,:,1])
        input_emb = torch.cat([emb_a, emb_b], dim=-1)
        h = input_emb.clone()
        
        relative_bias = self.get_relative_bias(seq_len, x.device)
        attn_mask = relative_bias.repeat(B, 1, 1)

        for _ in range(num_loops):
            h_new = self.shared_layer(h, src_mask=attn_mask)
            h = h_new + (self.input_gate * input_emb)
            
        return self.output_head(h)

# ==========================================
# 4. TRAINING & PLOTTING
# ==========================================
def train_multiple_models(
    model_configs,
    num_steps=2001,
    train_length=8,
    batch_size=64,
    lr=0.001,
    print_interval=500,
    eval_interval=None,
    eval_ood_length=None,
    eval_batch_size=100,
    device='cuda'
):
    """
    Train multiple transformer models on binary addition.

    Args:
        model_configs: List of dicts with keys:
            - 'model_class': Model class (BaseTransformer, BaseTransformer2, LoopedTransformer)
            - 'params': Dict of model initialization parameters
            - 'name': Display name for plots/logs
            - 'num_loops': Number of loops for inference (None for auto or non-looped models)
        num_steps: Number of training steps
        train_length: Length of numbers used during training. Can be:
                      - int: fixed length (e.g., 8)
                      - str: range format (e.g., "2to8") for curriculum learning
        batch_size: Batch size for training
        lr: Learning rate
        print_interval: Print loss every N steps
        eval_interval: Evaluate accuracy every N steps (None to disable)
        eval_ood_length: OOD length to track during training (None to disable)
        eval_batch_size: Batch size for evaluation
        device: 'cuda' or 'cpu'

    Returns:
        models_dict: Dict mapping model names to trained models
        loss_history: Dict mapping model names to list of (step, loss) tuples
        accuracy_history: Dict with 'train_length' and 'ood_length' keys, each containing
                         dict mapping model names to list of (step, accuracy) tuples
    """

    # Phase 1: Setup Models and Optimizers
    models_dict = {}
    optimizers_dict = {}

    for config in model_configs:
        model_class = config['model_class']
        params = config['params']
        name = config['name']

        # Instantiate model
        model = model_class(**params).to(device)
        models_dict[name] = model

        # Create optimizer
        optimizers_dict[name] = optim.Adam(model.parameters(), lr=lr)

    # Loss tracking
    loss_history = {name: [] for name in models_dict.keys()}

    # Accuracy tracking (if enabled)
    accuracy_history = None
    if eval_interval is not None and eval_ood_length is not None:
        accuracy_history = {
            'train_length': {name: [] for name in models_dict.keys()},
            'ood_length': {name: [] for name in models_dict.keys()}
        }

    criterion = nn.CrossEntropyLoss()

    # Parse train_length - either int or "XtoY" string format
    if isinstance(train_length, str) and 'to' in train_length:
        min_len, max_len = map(int, train_length.split('to'))
        use_curriculum = True
        train_length_display = f"{min_len}-{max_len}"
    else:
        min_len = max_len = int(train_length)
        use_curriculum = False
        train_length_display = str(train_length)

    # Phase 2: Training Loop
    print(f"--- Training Phase ({train_length_display}-Digit Numbers) ---")
    for step in range(num_steps + 1):
        # Sample length for this batch
        if use_curriculum:
            current_length = random.randint(min_len, max_len)
        else:
            current_length = min_len

        X, Y = generate_binary_addition_batch(batch_size, length=current_length, device=device)

        # Train each model
        for name, model in models_dict.items():
            optimizer = optimizers_dict[name]
            config = next(c for c in model_configs if c['name'] == name)

            optimizer.zero_grad()

            # Forward pass (handle num_loops for looped models)
            if config['num_loops'] is not None:
                out = model(X, num_loops=config['num_loops'])
            else:
                out = model(X)

            # Loss computation
            loss = criterion(out.reshape(-1, 10), Y.reshape(-1))
            loss.backward()
            optimizer.step()

            # Track loss
            if step % print_interval == 0:
                loss_history[name].append((step, loss.item()))

        # Print progress
        if step % print_interval == 0:
            loss_str = " | ".join([f"{name} Loss {loss_history[name][-1][1]:.3f}"
                                   for name in models_dict.keys()])
            print(f"Step {step}: {loss_str}")

        # Periodic evaluation (if enabled)
        if accuracy_history is not None and step % eval_interval == 0:
            with torch.no_grad():
                # Evaluate on training length (use max_len if curriculum)
                eval_train_length = max_len
                X_train_eval, Y_train_eval = generate_binary_addition_batch(
                    eval_batch_size, length=eval_train_length, max_seq_length=100, device=device
                )

                # Evaluate on OOD length
                X_ood_eval, Y_ood_eval = generate_binary_addition_batch(
                    eval_batch_size, length=eval_ood_length, max_seq_length=100, device=device
                )

                for name, model in models_dict.items():
                    config = next(c for c in model_configs if c['name'] == name)

                    # Evaluate on training length
                    if config['num_loops'] is not None:
                        out_train = model(X_train_eval, num_loops=eval_train_length + 4)
                    else:
                        out_train = model(X_train_eval)
                    predictions_train = out_train.argmax(-1)
                    acc_train = (predictions_train == Y_train_eval).float().mean().item()
                    accuracy_history['train_length'][name].append((step, acc_train))

                    # Evaluate on OOD length
                    if config['num_loops'] is not None:
                        out_ood = model(X_ood_eval, num_loops=eval_ood_length + 4)
                    else:
                        out_ood = model(X_ood_eval)
                    predictions_ood = out_ood.argmax(-1)
                    acc_ood = (predictions_ood == Y_ood_eval).float().mean().item()
                    accuracy_history['ood_length'][name].append((step, acc_ood))

    return models_dict, loss_history, accuracy_history

def evaluate_models(
    models_dict,
    model_configs,
    test_lengths=None,
    per_digit_position_length=None,
    batch_size=100,
    device='cuda'
):
    """
    Evaluate multiple trained models on binary addition task.

    Args:
        models_dict: Dict mapping model names to trained models
        model_configs: List of model config dicts (same as used in training)
        test_lengths: List of sequence lengths to test (default: [2,4,6,...,28])
        per_digit_position_length: Length to use for per-digit position accuracy analysis
                                   (default: None, will use first test_length)
        batch_size: Batch size for evaluation
        device: 'cuda' or 'cpu'

    Returns:
        results_dict: Dict with keys:
            - 'length_generalization': Dict mapping model names to per-digit accuracy lists
            - 'exact_match_accuracy': Dict mapping model names to exact match accuracy lists
            - 'per_digit_accuracy': Dict mapping model names to per-position accuracy arrays
            - 'test_lengths': List of test lengths used
            - 'per_digit_position_length': Length used for per-digit position analysis
    """
    print("\n--- Generalization Test ---")
    if test_lengths is None:
        test_lengths = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]

    if per_digit_position_length is None:
        per_digit_position_length = test_lengths[0]

    # Initialize result storage
    length_accuracies = {name: [] for name in models_dict.keys()}
    exact_match_accuracies = {name: [] for name in models_dict.keys()}
    per_digit_accuracies = {name: [] for name in models_dict.keys()}

    # Test each length
    for L in test_lengths:
        with torch.no_grad():
            X_test, Y_test = generate_binary_addition_batch(
                batch_size, length=L, max_seq_length=100, device=device
            )

            for name, model in models_dict.items():
                config = next(c for c in model_configs if c['name'] == name)

                # Inference with appropriate num_loops
                if config['num_loops'] is not None:
                    # Scale loops with length (L + buffer)
                    out = model(X_test, num_loops=L + 4)
                else:
                    out = model(X_test)

                # Overall accuracy (per-digit)
                predictions = out.argmax(-1)
                acc = (predictions == Y_test).float().mean().item()
                length_accuracies[name].append(acc)

                # Exact match accuracy (entire sequence must be correct)
                exact_matches = (predictions == Y_test).all(dim=1).float().mean().item()
                exact_match_accuracies[name].append(exact_matches)

                # Per-digit position accuracy (at specified length)
                if L == per_digit_position_length:
                    per_pos_acc = []
                    for pos in range(min(10, Y_test.shape[1])):
                        pos_acc = (predictions[:, pos] == Y_test[:, pos]).float().mean().item()
                        per_pos_acc.append(pos_acc)
                    per_digit_accuracies[name] = per_pos_acc

    # Build results dictionary
    results_dict = {
        'length_generalization': length_accuracies,
        'exact_match_accuracy': exact_match_accuracies,
        'per_digit_accuracy': per_digit_accuracies,
        'test_lengths': test_lengths,
        'per_digit_position_length': per_digit_position_length
    }

    return results_dict

def plot_training_losses(loss_history, model_names):
    """
    Create training loss curves for all models.

    Args:
        loss_history: Dict mapping model names to list of (step, loss) tuples
        model_names: List of model names (for ordering)
    """
    plt.figure(figsize=(10, 6))

    # Generate distinct colors and markers
    colors = plt.cm.tab10(range(len(model_names)))
    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']

    for idx, name in enumerate(model_names):
        steps, losses = zip(*loss_history[name])
        plt.plot(steps, losses,
                marker=markers[idx % len(markers)],
                color=colors[idx],
                label=name,
                linewidth=2,
                markersize=6,
                markevery=max(1, len(steps) // 10))

    plt.xlabel('Training Step', fontsize=12)
    plt.ylabel('Loss (log scale)', fontsize=12)
    plt.title('Training Loss Curves', fontsize=14, fontweight='bold')
    plt.yscale('log')
    plt.legend(loc='best')
    plt.grid(True, alpha=0.3, which='both')
    plt.tight_layout()
    plt.show()

def plot_evaluation_results(results_dict, train_length, model_names):
    """
    Create figure with three subplots:
    1. Per-digit position accuracy (positions 0-9)
    2. Length generalization - per-digit accuracy (lengths 2-28)
    3. Length generalization - exact match accuracy (lengths 2-28)

    Args:
        results_dict: Results dictionary with accuracy data (from evaluate_models)
        train_length: Training length for reference line in plots
        model_names: List of model names
    """
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))

    # Generate distinct colors and markers
    colors = plt.cm.tab10(range(len(model_names)))
    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']

    # Subplot 1: Per-Digit Position Accuracy
    for idx, name in enumerate(model_names):
        per_digit_acc = results_dict['per_digit_accuracy'][name]
        positions = list(range(len(per_digit_acc)))
        ax1.plot(positions, per_digit_acc,
                marker=markers[idx % len(markers)],
                color=colors[idx],
                label=name,
                linewidth=2,
                markersize=8)

    ax1.set_xlabel('Output Position (Digit Index)', fontsize=12)
    ax1.set_ylabel('Accuracy', fontsize=12)
    ax1.set_title('Per-Digit Position Accuracy', fontsize=14, fontweight='bold')
    # ax1.set_ylim([0, 1.0])
    ax1.legend(loc='best')
    ax1.grid(True, alpha=0.3)
    ax1.set_xticks(range(10))

    # Subplot 2: Length Generalization (Per-Digit)
    test_lengths = results_dict['test_lengths']
    for idx, name in enumerate(model_names):
        length_acc = results_dict['length_generalization'][name]
        ax2.plot(test_lengths, length_acc,
                marker=markers[idx % len(markers)],
                color=colors[idx],
                label=name,
                linewidth=2,
                markersize=6)

    ax2.axvline(x=train_length, color='gray', linestyle='--',
                linewidth=2, label='Training Cutoff', alpha=0.7)
    ax2.set_xlabel('Number of Digits', fontsize=12)
    ax2.set_ylabel('Per-Digit Accuracy', fontsize=12)
    ax2.set_title('Length Generalization (Per-Digit)', fontsize=14, fontweight='bold')
    # ax2.set_ylim([0, 1.0])
    ax2.legend(loc='best')
    ax2.grid(True, alpha=0.3)

    # Subplot 3: Length Generalization (Exact Match)
    for idx, name in enumerate(model_names):
        exact_match_acc = results_dict['exact_match_accuracy'][name]
        ax3.plot(test_lengths, exact_match_acc,
                marker=markers[idx % len(markers)],
                color=colors[idx],
                label=name,
                linewidth=2,
                markersize=6)

    ax3.axvline(x=train_length, color='gray', linestyle='--',
                linewidth=2, label='Training Cutoff', alpha=0.7)
    ax3.set_xlabel('Number of Digits', fontsize=12)
    ax3.set_ylabel('Exact Match Accuracy', fontsize=12)
    ax3.set_title('Length Generalization (Exact Match)', fontsize=14, fontweight='bold')
    # ax3.set_ylim([0, 1.0])
    ax3.legend(loc='best')
    ax3.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def plot_accuracy_during_training(accuracy_history, train_length, ood_length, model_names):
    """
    Plot accuracy on training length vs OOD length during training.

    Args:
        accuracy_history: Dict with 'train_length' and 'ood_length' keys, each containing
                         dict mapping model names to list of (step, accuracy) tuples
        train_length: Training length for labeling
        ood_length: OOD length for labeling
        model_names: List of model names
    """
    plt.figure(figsize=(10, 6))

    # Generate distinct colors and markers
    colors = plt.cm.tab10(range(len(model_names)))
    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']
    linestyles = ['-', '--']

    for idx, name in enumerate(model_names):
        # Plot training length accuracy (solid line)
        steps_train, accs_train = zip(*accuracy_history['train_length'][name])
        plt.plot(steps_train, accs_train,
                marker=markers[idx % len(markers)],
                color=colors[idx],
                linestyle=linestyles[0],
                label=f'{name} (Length={train_length})',
                linewidth=2,
                markersize=6,
                markevery=max(1, len(steps_train) // 10))

        # Plot OOD length accuracy (dashed line, same color)
        steps_ood, accs_ood = zip(*accuracy_history['ood_length'][name])
        plt.plot(steps_ood, accs_ood,
                marker=markers[idx % len(markers)],
                color=colors[idx],
                linestyle=linestyles[1],
                label=f'{name} (Length={ood_length}, OOD)',
                linewidth=2,
                markersize=6,
                markevery=max(1, len(steps_ood) // 10))

    plt.xlabel('Training Step', fontsize=12)
    plt.ylabel('Per-Digit Accuracy', fontsize=12)
    plt.title('Accuracy During Training: In-Distribution vs Out-of-Distribution',
              fontsize=14, fontweight='bold')
    # plt.ylim([0, 1.0])
    plt.legend(loc='best', fontsize=9)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# Example usage
model_configs = [
    # {
    #     'model_class': BaseTransformer,
    #     'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4},
    #     'name': 'Standard',
    #     'num_loops': None
    # },
    {
        'model_class': BaseTransformer2,
        'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
                  'use_rope': False, 'causal': False, 'rope_type': '2d'},
        'name': 'Nothing (for reference) ',
        'num_loops': None
    },
    {
        'model_class': BaseTransformer2,
        'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
                  'use_rope': True, 'causal': False, 'rope_type': '2d'},
        'name': 'RoPE Only',
        'num_loops': None
    },
    # {
    #     'model_class': BaseTransformer2,
    #     'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
    #               'use_rope': False, 'causal': True, 'rope_type': '2d'},
    #     'name': 'Causal Only',
    #     'num_loops': None
    # },    
    # {
    #     'model_class': BaseTransformer2,
    #     'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
    #               'use_rope': False, 'causal': False, 'rope_type': '2d', 'use_relative_bias': True, 'max_relative_dist': 4},
    #     'name': 'Relative Only',
    #     'num_loops': None
    # },   
    # {
    #     'model_class': BaseTransformer2,
    #     'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
    #               'use_rope': False, 'causal': True, 'rope_type': '2d', 'use_relative_bias': True, 'max_relative_dist': 4},
    #     'name': 'Relative Causal ',
    #     'num_loops': None
    # },   
    {        
        'model_class': BaseTransformer2,
        'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
                  'use_rope': True, 'causal': False, 'rope_type': '2d', 'use_relative_bias': True, 'max_relative_dist': 4},
        'name': 'RoPE Relative  ',
        'num_loops': None
    },   
    {
        'model_class': BaseTransformer2,
        'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
                  'use_rope': True, 'causal': True, 'rope_type': '2d'},
        'name': 'RoPE Causal',
        'num_loops': None
    },
    {
        'model_class': BaseTransformer2,
        'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
                  'use_rope': True, 'causal': True, 'rope_type': '2d', 'use_relative_bias': True, 'max_relative_dist': 4},
        'name': 'RoPE Causal Relative Bias',
        'num_loops': None
    },   
    # {
    #     'model_class': BaseTransformer2,
    #     'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
    #               'use_rope': True, 'causal': True, 'rope_type': '2d'},
    #     'name': 'RoPE Causal Looped 4',
    #     'num_loops': 4
    # },
    # {
    #     'model_class': BaseTransformer2,
    #     'params': {'d_model': 256, 'nhead': 4, 'num_layers': 4,
    #               'use_rope': True, 'causal': True, 'rope_type': '2d'},
    #     'name': 'RoPE Causal Looped 8',
    #     'num_loops': 8
    # }
]

models_dict, loss_history, accuracy_history = train_multiple_models(
    model_configs,
    num_steps=100001,
    eval_interval=500,  # Evaluate every 500 steps
    eval_ood_length=16  # Track OOD performance at length 16
)
model_names = [c['name'] for c in model_configs]

# Evaluate models
results_dict = evaluate_models(
    models_dict,
    model_configs,
    test_lengths=None,  # Use default [2,4,6,...,28]
    per_digit_position_length=8,  # Analyze position accuracy at training length
    device='cuda'
)

# Print results table
print("\n--- Results Table ---")
test_lengths = results_dict['test_lengths']
print(f"{'Length':<10} | " + " | ".join([f"{name:<15}" for name in model_names]))
print("-" * (12 + len(model_names) * 18))
for i, L in enumerate(test_lengths):
    acc_str = " | ".join([f"{results_dict['length_generalization'][name][i]:.2f}{'':<11}"
                          for name in model_names])
    print(f"{L:<10} | {acc_str}")

# Generate plots
results_dict['loss_history'] = loss_history  # Add loss history to results for plotting
plot_training_losses(loss_history, model_names)
plot_evaluation_results(results_dict, train_length=8, model_names=model_names)

# Plot accuracy during training (if tracking was enabled)
if accuracy_history is not None:
    plot_accuracy_during_training(accuracy_history, train_length=8, ood_length=16, model_names=model_names)