{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d01ab6",
   "metadata": {},
   "source": [
    "# Looped Transformers for Length Generalization\n",
    "This notebook compares looped transformers to simple ones and shows how they can generalize out of distribution to with addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eff431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e0c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input (X): tensor([[[0., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [1., 1.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 1.]]], device='cuda:0')\n",
      "Example Output (Y): tensor([[1, 0, 1, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0, 1, 0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. DATA GENERATION\n",
    "# ==========================================\n",
    "def generate_batch(batch_size, bits):\n",
    "    X, Y = [], []\n",
    "    for _ in range(batch_size):\n",
    "        a = random.randint(0, 2**bits - 1)\n",
    "        b = random.randint(0, 2**bits - 1)\n",
    "        c = a + b\n",
    "        \n",
    "        a_bin = [int(x) for x in format(a, f'0{bits}b')[::-1]]\n",
    "        b_bin = [int(x) for x in format(b, f'0{bits}b')[::-1]]\n",
    "        c_bin = [int(x) for x in format(c, f'0{bits}b')[::-1]]\n",
    "        \n",
    "        # Handle overflow/padding\n",
    "        if len(c_bin) < bits: \n",
    "            c_bin += [0] * (bits - len(c_bin))\n",
    "        c_bin = c_bin[:bits]\n",
    "        \n",
    "        x_seq = [[a_val, b_val] for a_val, b_val in zip(a_bin, b_bin)]\n",
    "        X.append(x_seq)\n",
    "        Y.append(c_bin)\n",
    "        \n",
    "    return torch.tensor(X).float().to(device), torch.tensor(Y).long().to(device)\n",
    "\n",
    "# Example usage:\n",
    "X_example, Y_example = generate_batch(2, bits=8)\n",
    "print(\"Example Input (X):\", X_example)\n",
    "print(\"Example Output (Y):\", Y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4ed377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers=4, max_len=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Absolute Positional Encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.1)\n",
    "                \n",
    "        # Transformer Encoder Block\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True, norm_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_head = nn.Linear(d_model, 2) # MLP that outputs 2 classes (0 or 1)\n",
    "\n",
    "    def forward(self, x, num_loops=None):\n",
    "        seq_len = x.shape[1]\n",
    "        # Add absolute position (up to current length)\n",
    "        # If seq_len > max_len, this crashes or requires slicing (we slice here)\n",
    "        pos = self.pos_embedding[:, :seq_len, :]\n",
    "        x = self.embedding(x) + pos\n",
    "        x = self.transformer(x)\n",
    "        return self.output_head(x)\n",
    "    \n",
    "    \n",
    "class UpdatedTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers=4, max_len=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Absolute Positional Encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.1)\n",
    "                \n",
    "        # Transformer Encoder Block\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True, norm_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_head = nn.Linear(d_model, 2) # MLP that outputs 2 classes (0 or 1)\n",
    "\n",
    "    def forward(self, x, num_loops=None):\n",
    "        seq_len = x.shape[1]\n",
    "        # Add absolute position (up to current length)\n",
    "        # If seq_len > max_len, this crashes or requires slicing (we slice here)\n",
    "        pos = self.pos_embedding[:, :seq_len, :]\n",
    "        x = self.embedding(x) + pos\n",
    "        x = self.transformer(x)\n",
    "        return self.output_head(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156145b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decimal_batch(batch_size, length):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    # We always define the sequence length as N + 1\n",
    "    # This guarantees space for the final carry.\n",
    "    seq_len = length + 1\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        # 1. Generate random numbers of 'length' digits\n",
    "        # e.g., if length=2, max is 99.\n",
    "        max_val = (10 ** length) - 1\n",
    "        a = random.randint(0, max_val)\n",
    "        b = random.randint(0, max_val)\n",
    "        c = a + b # Real sum (e.g., 99+99=198)\n",
    "        \n",
    "        # 2. Extract digits for the FULL sequence (length + 1)\n",
    "        # We grab digits 0 to 'length'.\n",
    "        # If the number is small (e.g. 50+20=70), the last digit will naturally be 0.\n",
    "        a_seq = [(a // (10**i)) % 10 for i in range(seq_len)]\n",
    "        b_seq = [(b // (10**i)) % 10 for i in range(seq_len)]\n",
    "        c_seq = [(c // (10**i)) % 10 for i in range(seq_len)]\n",
    "        \n",
    "        # 3. Stack inputs\n",
    "        # The last position of input will always be [0, 0]\n",
    "        # The AI sees this and thinks \"Ah, a blank space for me to write the carry.\"\n",
    "        x_seq = [[d_a, d_b] for d_a, d_b in zip(a_seq, b_seq)]\n",
    "        \n",
    "        X.append(x_seq)\n",
    "        Y.append(c_seq)\n",
    "        \n",
    "    return torch.tensor(X).long().to(device), torch.tensor(Y).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4835f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseTransformer.__init__() missing 1 required positional argument: 'nhead'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m     plt.grid(\u001b[38;5;28;01mTrue\u001b[39;00m, alpha=\u001b[32m0.3\u001b[39m)\n\u001b[32m     75\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mtrain_and_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_and_compare\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m NHEAD = \u001b[32m4\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. Setup Models\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m std_model = \u001b[43mBaseTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNHEAD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      7\u001b[39m loop_model = UpdatedTransformer(D_MODEL, NHEAD, num_layers=\u001b[32m4\u001b[39m).to(device) \u001b[38;5;66;03m# Window=4\u001b[39;00m\n\u001b[32m      9\u001b[39m optimizers = {\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mStandard\u001b[39m\u001b[33m\"\u001b[39m: optim.Adam(std_model.parameters(), lr=\u001b[32m0.001\u001b[39m),\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLooped\u001b[39m\u001b[33m\"\u001b[39m: optim.Adam(loop_model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m     12\u001b[39m }\n",
      "\u001b[31mTypeError\u001b[39m: BaseTransformer.__init__() missing 1 required positional argument: 'nhead'"
     ]
    }
   ],
   "source": [
    "def train_and_compare():\n",
    "    D_MODEL = 128 # Slightly larger for decimal complexity\n",
    "    NHEAD = 4\n",
    "    \n",
    "    # 1. Setup Models\n",
    "    std_model = BaseTransformer(D_MODEL, NHEAD, num_layers=4).to(device)\n",
    "    loop_model = UpdatedTransformer(D_MODEL, NHEAD, num_layers=4).to(device) # Window=4\n",
    "    \n",
    "    optimizers = {\n",
    "        \"Standard\": optim.Adam(std_model.parameters(), lr=0.001),\n",
    "        \"Looped\": optim.Adam(loop_model.parameters(), lr=0.001)\n",
    "    }\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"--- Training Phase (8-Digit Numbers) ---\")\n",
    "    # Train both for 2000 steps\n",
    "    for step in range(2001):\n",
    "        X, Y = generate_decimal_batch(64, length=8)\n",
    "        \n",
    "        # Train Standard\n",
    "        opt = optimizers[\"Standard\"]\n",
    "        opt.zero_grad()\n",
    "        out = std_model(X)\n",
    "        loss_std = criterion(out.reshape(-1, 10), Y.reshape(-1))\n",
    "        loss_std.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # Train Looped\n",
    "        opt = optimizers[\"Looped\"]\n",
    "        opt.zero_grad()\n",
    "        out = loop_model(X, num_loops=None) # 8 digits + buffer\n",
    "        loss_loop = criterion(out.reshape(-1, 10), Y.reshape(-1))\n",
    "        loss_loop.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f\"Step {step}: Std Loss {loss_std.item():.3f} | Loop Loss {loss_loop.item():.3f}\")\n",
    "\n",
    "    print(\"\\n--- Generalization Test (Length 8 to 24) ---\")\n",
    "    lengths = [8, 12, 16, 20, 24]\n",
    "    std_accs = []\n",
    "    loop_accs = []\n",
    "    \n",
    "    for L in lengths:\n",
    "        with torch.no_grad():\n",
    "            X_test, Y_test = generate_decimal_batch(100, length=L)\n",
    "            \n",
    "            # Test Standard\n",
    "            out_std = std_model(X_test)\n",
    "            acc_s = (out_std.argmax(-1) == Y_test).float().mean().item()\n",
    "            std_accs.append(acc_s)\n",
    "            \n",
    "            # Test Looped (Scale loops with length)\n",
    "            out_loop = loop_model(X_test, num_loops=L + 5)\n",
    "            acc_l = (out_loop.argmax(-1) == Y_test).float().mean().item()\n",
    "            loop_accs.append(acc_l)\n",
    "            \n",
    "    # Print Table\n",
    "    print(f\"{'Length':<10} | {'Standard Acc':<15} | {'Looped Acc':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "    for i, L in enumerate(lengths):\n",
    "        print(f\"{L:<10} | {std_accs[i]:.2f}{'':<11} | {loop_accs[i]:.2f}\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(lengths, std_accs, 'r-o', label='Standard (Memorization)')\n",
    "    plt.plot(lengths, loop_accs, 'g-o', label='Looped (Abstraction)')\n",
    "    plt.axvline(x=8, color='gray', linestyle='--', label='Training Cutoff')\n",
    "    plt.title(\"Decimal Addition: Generalization to Long Numbers\")\n",
    "    plt.xlabel(\"Number of Digits\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "train_and_compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c48a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
