{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d01ab6",
   "metadata": {},
   "source": [
    "# Looped Transformers for Length Generalization\n",
    "This notebook compares looped transformers to simple ones and shows how they can generalize out of distribution to with addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eff431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e0c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input (X): tensor([[[0., 1.],\n",
      "         [0., 0.],\n",
      "         [1., 1.],\n",
      "         [0., 0.],\n",
      "         [0., 1.],\n",
      "         [1., 1.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]]], device='cuda:0')\n",
      "Example Output (Y): tensor([[1, 0, 0, 1, 1, 0, 0, 1],\n",
      "        [0, 1, 1, 1, 1, 0, 1, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. DATA GENERATION\n",
    "# ==========================================\n",
    "def generate_batch(batch_size, bits):\n",
    "    X, Y = [], []\n",
    "    for _ in range(batch_size):\n",
    "        a = random.randint(0, 2**bits - 1)\n",
    "        b = random.randint(0, 2**bits - 1)\n",
    "        c = a + b\n",
    "        \n",
    "        a_bin = [int(x) for x in format(a, f'0{bits}b')[::-1]]\n",
    "        b_bin = [int(x) for x in format(b, f'0{bits}b')[::-1]]\n",
    "        c_bin = [int(x) for x in format(c, f'0{bits}b')[::-1]]\n",
    "        \n",
    "        # Handle overflow/padding\n",
    "        if len(c_bin) < bits: \n",
    "            c_bin += [0] * (bits - len(c_bin))\n",
    "        c_bin = c_bin[:bits]\n",
    "        \n",
    "        x_seq = [[a_val, b_val] for a_val, b_val in zip(a_bin, b_bin)]\n",
    "        X.append(x_seq)\n",
    "        Y.append(c_bin)\n",
    "        \n",
    "    return torch.tensor(X).float().to(device), torch.tensor(Y).long().to(device)\n",
    "\n",
    "# Example usage:\n",
    "X_example, Y_example = generate_batch(2, bits=8)\n",
    "print(\"Example Input (X):\", X_example)\n",
    "print(\"Example Output (Y):\", Y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. STANDARD TRANSFORMER (Baseline)\n",
    "# ==========================================\n",
    "class StandardTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers=4, max_len=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Absolute Positional Encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.1)\n",
    "        \n",
    "        # Transformer Encoder Block\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True, norm_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_head = nn.Linear(d_model, 2) # MLP that outputs 2 classes (0 or 1)\n",
    "\n",
    "    def forward(self, x, num_loops=None):\n",
    "        seq_len = x.shape[1]\n",
    "        # Add absolute position (up to current length)\n",
    "        # If seq_len > max_len, this crashes or requires slicing (we slice here)\n",
    "        pos = self.pos_embedding[:, :seq_len, :]\n",
    "        x = self.embedding(x) + pos\n",
    "        x = self.transformer(x)\n",
    "        return self.output_head(x)\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOOPED TRANSFORMER\n",
    "# ==========================================\n",
    "class LoopedTransformerRelative(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, max_relative_dist=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Learned Relative Bias Table\n",
    "        self.max_relative_dist = max_relative_dist\n",
    "        # Range [-dist, +dist] -> size is 2*dist + 1\n",
    "        self.relative_bias_table = nn.Parameter(torch.Tensor(1, nhead, 2 * max_relative_dist + 1))\n",
    "        nn.init.xavier_uniform_(self.relative_bias_table)\n",
    "        \n",
    "        self.shared_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.output_head = nn.Linear(d_model, 2)\n",
    "        self.input_gate = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def get_relative_bias(self, seq_len, device):\n",
    "        range_vec = torch.arange(seq_len, device=device)\n",
    "        distance_mat = range_vec[None, :] - range_vec[:, None]\n",
    "        \n",
    "        # CRITICAL: Clamp distance to Trained Window (e.g., -4 to 4)\n",
    "        distance_mat_clamped = torch.clamp(distance_mat, -self.max_relative_dist, self.max_relative_dist)\n",
    "        final_indices = distance_mat_clamped + self.max_relative_dist\n",
    "        \n",
    "        bias = self.relative_bias_table[0, :, final_indices.long()] # [nhead, L, L]\n",
    "        \n",
    "        # Causal Mask (Prevent cheating by looking ahead)\n",
    "        causal_mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)\n",
    "        return bias + causal_mask.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x, num_loops=None):\n",
    "        B, seq_len, _ = x.shape\n",
    "        # Adaptive Depth: If input is longer, think longer.\n",
    "        if num_loops is None:\n",
    "            num_loops = seq_len + 5 \n",
    "\n",
    "        input_emb = self.embedding(x)\n",
    "        h = input_emb.clone()\n",
    "        \n",
    "        # Get Bias\n",
    "        relative_bias = self.get_relative_bias(seq_len, x.device)\n",
    "        attn_mask = relative_bias.repeat(B, 1, 1) # Repeat for batch\n",
    "\n",
    "        for _ in range(num_loops):\n",
    "            h_new = self.shared_layer(h, src_mask=attn_mask)\n",
    "            # Input Injection (Skip Connection to Input)\n",
    "            h = h_new + (self.input_gate * input_emb)\n",
    "            \n",
    "        return self.output_head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c95983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. EXPERIMENT RUNNER\n",
    "# ==========================================\n",
    "def train_model(model_class, name, kwargs):\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    model = model_class(**kwargs).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train on 8-bit ONLY\n",
    "    for step in range(2501):\n",
    "        optimizer.zero_grad()\n",
    "        # t = torch.randint(0, 4, (1,)).item()\n",
    "        X, Y = generate_batch(64, bits=8)\n",
    "        \n",
    "        # Use 12 loops for training (8 bits + buffer)\n",
    "        # Standard model ignores this argument\n",
    "        output = model(X, num_loops=12) \n",
    "        \n",
    "        loss = criterion(output.reshape(-1, 2), Y.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            acc = (output.argmax(-1) == Y).float().mean()\n",
    "            print(f\"Step {step}: Loss {loss.item():.4f}, Train Acc (8-bit): {acc:.2f}\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "def evaluate_length_generalization(model, name, test_lengths):\n",
    "    accuracies = []\n",
    "    print(f\"Evaluating {name}...\", end=\" \")\n",
    "    for L in test_lengths:\n",
    "        with torch.no_grad():\n",
    "            X, Y = generate_batch(100, bits=L)\n",
    "            # Dynamic Loops: Scale loops with problem length L\n",
    "            # Standard model will ignore this and use fixed layers\n",
    "            output = model(X, num_loops=L + 5) \n",
    "            acc = (output.argmax(-1) == Y).float().mean().item()\n",
    "            accuracies.append(acc)\n",
    "    print(\"Done.\")\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b68624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Standard Transformer ---\n",
      "Step 0: Loss 1.4052, Train Acc (8-bit): 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ous\\miniconda3\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Loss 0.0078, Train Acc (8-bit): 1.00\n",
      "Step 1000: Loss 0.0083, Train Acc (8-bit): 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5. EXECUTION & PLOTTING\n",
    "# ==========================================\n",
    "\n",
    "# Parameters\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "INPUT_DIM = 2\n",
    "\n",
    "# 1. Train Standard (Fixed Depth = 4 Layers, Absolute Pos)\n",
    "std_model = train_model(\n",
    "    StandardTransformer, \n",
    "    \"Standard Transformer\", \n",
    "    {\"input_dim\": INPUT_DIM, \"d_model\": D_MODEL, \"nhead\": NHEAD, \"num_layers\": 4}\n",
    ")\n",
    "\n",
    "# 2. Train Looped (Relative Pos, Window=4)\n",
    "# Note: max_relative_dist=4 ensures we cover the window within 8-bit training\n",
    "looped_model = train_model(\n",
    "    LoopedTransformerRelative, \n",
    "    \"Looped Transformer\", \n",
    "    {\"input_dim\": INPUT_DIM, \"d_model\": D_MODEL, \"nhead\": NHEAD, \"max_relative_dist\": 4}\n",
    ")\n",
    "\n",
    "# 3. Test Generalization (From 8 bits up to 48 bits)\n",
    "test_lengths = [8, 12, 16, 20, 24, 32, 40, 48]\n",
    "std_acc = evaluate_length_generalization(std_model, \"Standard\", test_lengths)\n",
    "loop_acc = evaluate_length_generalization(looped_model, \"Looped\", test_lengths)\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_lengths, std_acc, 'o--', label='Standard Transformer (Absolute Pos)', color='red', linewidth=2)\n",
    "plt.plot(test_lengths, loop_acc, 'o-', label='Looped Transformer (Relative + Window)', color='green', linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "plt.axvline(x=8, color='gray', linestyle=':', label='Training Cutoff (8-bit)')\n",
    "plt.title('The \"Grokking\" Gap: Interpolation vs Extrapolation', fontsize=14)\n",
    "plt.xlabel('Sequence Length (Number of Bits)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim(-0.05, 1.1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Show or Save\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Lengths: {test_lengths}\")\n",
    "print(f\"Standard Acc: {std_acc}\")\n",
    "print(f\"Looped Acc:   {loop_acc}\")\n",
    "\n",
    "plt.show() # If running in notebook\n",
    "# plt.savefig(\"generalization_plot.png\") # If running in script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ed377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
